{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae49e5ad-7735-4d7e-abd0-a358e4d12888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (15308, 23)\n",
      "Test data shape: (914, 23)\n",
      "\n",
      "Training data preview:\n",
      "   row_id agent_code  agent_age agent_join_month first_policy_sold_month  \\\n",
      "0       1   455ca878         45         2/1/2021                9/1/2023   \n",
      "1       2   c823ce77         48         4/1/2022                2/1/2024   \n",
      "2       3   62154055         53         5/1/2020                9/1/2023   \n",
      "3       4   c58bfa6e         44         7/1/2019                3/1/2022   \n",
      "4       5   b1e5f770         20         9/1/2020                2/1/2023   \n",
      "\n",
      "  year_month  unique_proposals_last_7_days  unique_proposals_last_15_days  \\\n",
      "0   1/1/2023                             3                              6   \n",
      "1   1/1/2023                             1                              4   \n",
      "2   1/1/2023                             3                              5   \n",
      "3   1/1/2023                             1                              0   \n",
      "4   1/1/2023                             0                              6   \n",
      "\n",
      "   unique_proposals_last_21_days  unique_proposal  ...  unique_quotations  \\\n",
      "0                              2               12  ...                  9   \n",
      "1                             12               21  ...                 14   \n",
      "2                              5               17  ...                 14   \n",
      "3                             12               17  ...                 11   \n",
      "4                             10               17  ...                 13   \n",
      "\n",
      "   unique_customers_last_7_days  unique_customers_last_15_days  \\\n",
      "0                             3                              4   \n",
      "1                             4                              3   \n",
      "2                             3                              7   \n",
      "3                             2                              2   \n",
      "4                             3                             10   \n",
      "\n",
      "   unique_customers_last_21_days  unique_customers  new_policy_count  \\\n",
      "0                              8                15                 0   \n",
      "1                             10                17                25   \n",
      "2                             11                21                26   \n",
      "3                              9                13                18   \n",
      "4                              6                19                20   \n",
      "\n",
      "   ANBP_value  net_income  number_of_policy_holders  \\\n",
      "0           0       93313                        30   \n",
      "1     1071450      164804                         0   \n",
      "2     1601210      426690                        84   \n",
      "3      283230       47793                        39   \n",
      "4     1957680      562239                        19   \n",
      "\n",
      "   number_of_cash_payment_policies  \n",
      "0                              162  \n",
      "1                              175  \n",
      "2                               78  \n",
      "3                              144  \n",
      "4                               40  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Missing values in training data:\n",
      "row_id                             0\n",
      "agent_code                         0\n",
      "agent_age                          0\n",
      "agent_join_month                   0\n",
      "first_policy_sold_month            0\n",
      "year_month                         0\n",
      "unique_proposals_last_7_days       0\n",
      "unique_proposals_last_15_days      0\n",
      "unique_proposals_last_21_days      0\n",
      "unique_proposal                    0\n",
      "unique_quotations_last_7_days      0\n",
      "unique_quotations_last_15_days     0\n",
      "unique_quotations_last_21_days     0\n",
      "unique_quotations                  0\n",
      "unique_customers_last_7_days       0\n",
      "unique_customers_last_15_days      0\n",
      "unique_customers_last_21_days      0\n",
      "unique_customers                   0\n",
      "new_policy_count                   0\n",
      "ANBP_value                         0\n",
      "net_income                         0\n",
      "number_of_policy_holders           0\n",
      "number_of_cash_payment_policies    0\n",
      "dtype: int64\n",
      "\n",
      "Features used in the model:\n",
      "['agent_age', 'unique_proposals_last_7_days', 'unique_proposals_last_15_days', 'unique_proposals_last_21_days', 'unique_proposal', 'unique_quotations_last_7_days', 'unique_quotations_last_15_days', 'unique_quotations_last_21_days', 'unique_quotations', 'unique_customers_last_7_days', 'unique_customers_last_15_days', 'unique_customers_last_21_days', 'unique_customers', 'ANBP_value', 'net_income', 'number_of_policy_holders', 'number_of_cash_payment_policies', 'agent_experience_months', 'months_since_first_policy', 'current_month', 'current_year', 'join_month', 'join_year', 'proposal_to_quotation_ratio', 'quotation_to_customer_ratio', 'cash_payment_ratio', 'proposal_decline_7_15', 'proposal_decline_15_21', 'quotation_decline_7_15', 'quotation_decline_15_21', 'customer_decline_7_15', 'customer_decline_15_21']\n",
      "\n",
      "Train features shape: (12246, 32)\n",
      "Validation features shape: (3062, 32)\n",
      "\n",
      "Class distribution in training set:\n",
      "1    0.900049\n",
      "0    0.099951\n",
      "Name: proportion, dtype: float64\n",
      "[0]\tvalidation_0-logloss:0.28083\tvalidation_1-logloss:0.28098\n",
      "[99]\tvalidation_0-logloss:0.00272\tvalidation_1-logloss:0.00488\n",
      "\n",
      "Cross-validation results:\n",
      "accuracy: 0.9984 ± 0.0007\n",
      "precision: 0.9989 ± 0.0003\n",
      "recall: 0.9993 ± 0.0008\n",
      "f1: 0.9991 ± 0.0004\n",
      "roc_auc: 0.9999 ± 0.0001\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.9984\n",
      "Precision: 0.9993\n",
      "Recall: 0.9989\n",
      "F1 Score: 0.9991\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       306\n",
      "           1       1.00      1.00      1.00      2756\n",
      "\n",
      "    accuracy                           1.00      3062\n",
      "   macro avg       0.99      1.00      1.00      3062\n",
      "weighted avg       1.00      1.00      1.00      3062\n",
      "\n",
      "\n",
      "Submission file created: xgboost_submission.csv\n",
      "\n",
      "Best threshold: 0.30 with F1 score: 0.9993\n",
      "\n",
      "Optimized submission file created: xgboost_submission_optimized.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train_storming_round.csv')\n",
    "test_data = pd.read_csv('test_storming_round.csv')\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Preview the training data\n",
    "print(\"\\nTraining data preview:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Create target column first, before any feature engineering\n",
    "# to prevent target leakage\n",
    "def create_target(df):\n",
    "    if 'new_policy_count' in df.columns:\n",
    "        # Create target column: if new_policy_count is 0, target = 0, else target = 1\n",
    "        return np.where(df['new_policy_count'] == 0, 0, 1)\n",
    "    return None\n",
    "\n",
    "# Create target for training data only\n",
    "target = create_target(train_data)\n",
    "\n",
    "# Data preprocessing - modified to avoid using target-related information\n",
    "def preprocess_data(df, is_training=True):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime format\n",
    "    date_columns = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "    for col in date_columns:\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = pd.to_datetime(processed_df[col])\n",
    "    \n",
    "    # Create features from date columns\n",
    "    processed_df['agent_experience_months'] = ((processed_df['year_month'].dt.year - processed_df['agent_join_month'].dt.year) * 12 + \n",
    "                                    (processed_df['year_month'].dt.month - processed_df['agent_join_month'].dt.month))\n",
    "    \n",
    "    # Calculate months since first policy sold\n",
    "    processed_df['months_since_first_policy'] = np.where(pd.notnull(processed_df['first_policy_sold_month']),\n",
    "                                             ((processed_df['year_month'].dt.year - processed_df['first_policy_sold_month'].dt.year) * 12 + \n",
    "                                             (processed_df['year_month'].dt.month - processed_df['first_policy_sold_month'].dt.month)),\n",
    "                                             -1)  # -1 for agents who haven't sold a policy yet\n",
    "    \n",
    "    # Extract month and year features\n",
    "    processed_df['current_month'] = processed_df['year_month'].dt.month\n",
    "    processed_df['current_year'] = processed_df['year_month'].dt.year\n",
    "    processed_df['join_month'] = processed_df['agent_join_month'].dt.month\n",
    "    processed_df['join_year'] = processed_df['agent_join_month'].dt.year\n",
    "    \n",
    "    # Calculate ratios and other derived features WITHOUT using new_policy_count\n",
    "    processed_df['proposal_to_quotation_ratio'] = processed_df['unique_proposal'] / (processed_df['unique_quotations'] + 1)\n",
    "    processed_df['quotation_to_customer_ratio'] = processed_df['unique_quotations'] / (processed_df['unique_customers'] + 1)\n",
    "    \n",
    "    # Remove features that use new_policy_count to prevent target leakage\n",
    "    # processed_df['policy_per_customer'] = processed_df['new_policy_count'] / (processed_df['unique_customers'] + 1)\n",
    "    # processed_df['average_premium'] = processed_df['ANBP_value'] / (processed_df['new_policy_count'] + 1)\n",
    "    \n",
    "    # Alternative features that don't use the target\n",
    "    if 'number_of_policy_holders' in processed_df.columns:\n",
    "        processed_df['cash_payment_ratio'] = processed_df['number_of_cash_payment_policies'] / (processed_df['number_of_policy_holders'] + 1)\n",
    "    \n",
    "    # Calculate activity decline features\n",
    "    processed_df['proposal_decline_7_15'] = processed_df['unique_proposals_last_7_days'] - processed_df['unique_proposals_last_15_days']\n",
    "    processed_df['proposal_decline_15_21'] = processed_df['unique_proposals_last_15_days'] - processed_df['unique_proposals_last_21_days']\n",
    "    processed_df['quotation_decline_7_15'] = processed_df['unique_quotations_last_7_days'] - processed_df['unique_quotations_last_15_days']\n",
    "    processed_df['quotation_decline_15_21'] = processed_df['unique_quotations_last_15_days'] - processed_df['unique_quotations_last_21_days']\n",
    "    processed_df['customer_decline_7_15'] = processed_df['unique_customers_last_7_days'] - processed_df['unique_customers_last_15_days']\n",
    "    processed_df['customer_decline_15_21'] = processed_df['unique_customers_last_15_days'] - processed_df['unique_customers_last_21_days']\n",
    "    \n",
    "    # Drop original date columns\n",
    "    drop_cols = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "    \n",
    "    # Remove new_policy_count to prevent target leakage\n",
    "    if 'new_policy_count' in processed_df.columns:\n",
    "        drop_cols.append('new_policy_count')\n",
    "    \n",
    "    processed_df = processed_df.drop(columns=drop_cols, errors='ignore')\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Preprocess train and test data\n",
    "train_processed = preprocess_data(train_data)\n",
    "test_processed = preprocess_data(test_data, is_training=False)\n",
    "\n",
    "# Define features\n",
    "X = train_processed.drop(['row_id', 'agent_code'], axis=1, errors='ignore')\n",
    "y = target  # Use the target we created earlier\n",
    "\n",
    "# Print feature set to verify no leakage\n",
    "print(\"\\nFeatures used in the model:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Split data for training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "print(\"\\nTrain features shape:\", X_train.shape)\n",
    "print(\"Validation features shape:\", X_valid.shape)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = []\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    X_cv_train, X_cv_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_cv_train, y_cv_valid = y[train_idx], y[valid_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
    "    X_cv_valid_scaled = scaler.transform(X_cv_valid)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(X_cv_train, y_cv_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_cv_pred = model.predict(X_cv_valid)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cv_scores.append({\n",
    "        'accuracy': accuracy_score(y_cv_valid, y_cv_pred),\n",
    "        'precision': precision_score(y_cv_valid, y_cv_pred),\n",
    "        'recall': recall_score(y_cv_valid, y_cv_pred),\n",
    "        'f1': f1_score(y_cv_valid, y_cv_pred),\n",
    "        'roc_auc': roc_auc_score(y_cv_valid, model.predict_proba(X_cv_valid)[:, 1])\n",
    "    })\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    values = [score[metric] for score in cv_scores]\n",
    "    print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred_proba = xgb_model.predict_proba(X_valid)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_valid, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_valid, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_valid, y_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_valid, y_pred_proba):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_valid, y_pred_proba)\n",
    "plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_valid, y_pred_proba):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "xgb.plot_importance(xgb_model, max_num_features=20, height=0.5)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Prepare test data for prediction\n",
    "X_test = test_processed.drop(['row_id', 'agent_code'], axis=1, errors='ignore')\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_data['row_id'],\n",
    "    'target_column': test_pred\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('xgboost_submission.csv', index=False)\n",
    "print(\"\\nSubmission file created: xgboost_submission.csv\")\n",
    "\n",
    "# Additional threshold optimization (for better performance)\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_valid, y_pred_thresh)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nBest threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# Apply best threshold to test predictions\n",
    "test_pred_optimized = (test_pred_proba >= best_threshold).astype(int)\n",
    "submission_optimized = pd.DataFrame({\n",
    "    'row_id': test_data['row_id'],\n",
    "    'target_column': test_pred_optimized\n",
    "})\n",
    "\n",
    "# Save optimized submission file\n",
    "submission_optimized.to_csv('submission2.csv', index=False)\n",
    "print(\"\\nOptimized submission file created: xgboost_submission_optimized.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
